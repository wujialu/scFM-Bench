import pandas as pd 
import torch
import os
import glob
import numpy as np 
from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler
import scanpy


def train_domian_loaders_l(args):
    """
    Create training dataloaders

    Args:
        args (object) : args object generated by opt_util.py
        shuffle_state (bool) : Whether to shuffle the data

    Returns:
        a list of dataloader
    """
    SPL_PATH = args.train_dir
    NEED_ROWS = args.NEED_ROWS
    batch_size = args.batch_size
    TRAIN_GENE_LIST = os.path.join(args.output,args.genelist_outname)
    need_col_num=args.gene_num
    label_str = args.label_str
    gene_list = pd.read_csv(TRAIN_GENE_LIST,header=None,index_col=0)
    train_loader_l = []
    raw_df_l = []
    adata_l = []
    for data_spl_file in sorted(glob.glob(os.path.join(SPL_PATH,'*.*'))):
        # leave-one-domain-out validation
        if args.val_domain not in data_spl_file:
            if data_spl_file.endswith('.h5ad'):
                adata = scanpy.read_h5ad(data_spl_file)
                raw_df = adata.to_df()
                raw_df = pd.merge(gene_list,raw_df.T,how='left',left_index=True,right_index=True).fillna(0)
                raw_df = raw_df[~raw_df.index.duplicated()]
                raw_df = raw_df.loc[gene_list.index] # re-order
                raw_df = pd.merge(raw_df.T,pd.DataFrame(adata.obs[label_str]),left_index=True,right_index=True,how='inner')
            else: 
                if data_spl_file.endswith('.tsv'):
                    raw_df = pd.read_csv(data_spl_file,index_col=0,sep='\t')
                elif data_spl_file.endswith('.csv'):
                    raw_df = pd.read_csv(data_spl_file,index_col=0,sep=',')
                raw_df = pd.merge(gene_list,raw_df,how='left',left_index=True,right_index=True).fillna(0)
                raw_df = raw_df[~raw_df.index.duplicated()]
                raw_df = raw_df.loc[gene_list.index]
                raw_df = raw_df.T

            # Balanced sampling
            Y = raw_df.loc[:,label_str].to_numpy()
            num_neg, num_pos = (Y==0).sum(), (Y==1).sum()
            np.random.seed(42)
            if num_neg > num_pos:
                drop_indices = np.random.choice(np.where(Y==0)[0], num_neg-num_pos, replace=False)
            elif num_pos > num_neg:
                drop_indices = np.random.choice(np.where(Y==1)[0], num_pos-num_neg, replace=False)
            all_indices = np.arange(len(Y))
            keep_indices = np.setdiff1d(all_indices, drop_indices)
            raw_df_l.append(raw_df.iloc[keep_indices, :])
            adata_l.append(adata[keep_indices, :].copy())
            # NEED_ROWS = max(NEED_ROWS, raw_df.shape[0])
    
    # combine raw_df and pretrained embeddings
    for raw_df, adata in zip(raw_df_l, adata_l):
        # if NEED_ROWS > raw_df.shape[0]:
        #     raw_df = pd.DataFrame(np.repeat(raw_df.values,NEED_ROWS/(raw_df.shape[0]),axis=0),columns=raw_df.columns)
        # else:
        #     raw_df = raw_df.sample(NEED_ROWS,random_state=42)
        # X = raw_df.iloc[:,:need_col_num].to_numpy() #! default: gene_list=gene_num+1(label_col)
        # Y = raw_df.loc[:,label_str].to_numpy()

        if args.input_features == "X":
            X = raw_df.iloc[:,:need_col_num].to_numpy()
        else:
            X = adata.obsm[args.input_features]
        Y = raw_df.loc[:,label_str].to_numpy()

        raw_set = TensorDataset(torch.from_numpy(X).float(),torch.from_numpy(Y).float())
        train_set = raw_set
        train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, drop_last=True)
        train_loader_l.append(train_loader)
    return train_loader_l, X.shape[1]

def val_domian_loaders_l(args):
    """
    Create validating dataloaders

    Args:
        args (object) : args object generated by opt_util.py
        shuffle_state (bool) : Whether to shuffle the data

    Returns:
        a dict of validating dataloaders
    """
    SPL_PATH = args.val_dir
    NEED_ROWS = args.NEED_ROWS
    batch_size = args.batch_size
    TRAIN_GENE_LIST = os.path.join(args.output,args.genelist_outname)
    need_col_num=args.gene_num
    label_str = args.label_str
    gene_list = pd.read_csv(TRAIN_GENE_LIST,header=None,index_col=0)
    val_loader_dict = {}
    for data_spl_file in sorted(glob.glob(os.path.join(SPL_PATH,'*.*'))):
        if args.val_domain in data_spl_file:
            if data_spl_file.endswith('.h5ad'):
                adata = scanpy.read_h5ad(data_spl_file)
                raw_df = adata.to_df()
                raw_df = pd.merge(gene_list,raw_df.T,how='left',left_index=True,right_index=True).fillna(0)
                raw_df = raw_df[~raw_df.index.duplicated()]
                raw_df = raw_df.loc[gene_list.index] # re-order
                raw_df = pd.merge(raw_df.T,pd.DataFrame(adata.obs[label_str]),left_index=True,right_index=True,how='inner')
            else: 
                if data_spl_file.endswith('.tsv'):
                    raw_df = pd.read_csv(data_spl_file,index_col=0,sep='\t')
                elif data_spl_file.endswith('.csv'):
                    raw_df = pd.read_csv(data_spl_file,index_col=0,sep=',')
                raw_df = pd.merge(gene_list,raw_df,how='left',left_index=True,right_index=True).fillna(0)
                raw_df = raw_df[~raw_df.index.duplicated()]
                raw_df = raw_df.loc[gene_list.index]
                raw_df = raw_df.T

            # if NEED_ROWS > raw_df.shape[0] :
            #     raw_df = pd.DataFrame(np.repeat(raw_df.values,NEED_ROWS/(raw_df.shape[0]),axis=0),columns=raw_df.columns)
            # else :
            #     raw_df = raw_df.sample(NEED_ROWS,random_state=42)
            # X = raw_df.iloc[:,:need_col_num].to_numpy()
            # Y = raw_df.loc[:,label_str].to_numpy()

            if args.input_features == "X":
                X = raw_df.iloc[:,:need_col_num].to_numpy()
            else:
                X = adata.obsm[args.input_features]
            Y = raw_df.loc[:,label_str].to_numpy()

            raw_set = TensorDataset(torch.from_numpy(X).float(),torch.from_numpy(Y).float())
            val_set = raw_set
            val_loader = DataLoader(dataset=val_set, batch_size = batch_size, shuffle=False, drop_last=False)
            val_loader_dict[os.path.basename(data_spl_file).split('.')[0]] = val_loader
    print(val_loader_dict)
    return val_loader_dict